---
title: "Tree Methods Lab 2022"
author: "Ander Wilson"
date: "7/25/2022"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE)
knit_hooks$set(purl = hook_purl) 
```

## Lab overview

In this lab we conduct an analysis of the NHANES data using two approaches:

- Random forests with covariates regressed out prior to fitting the random forest model.
- BART with covariates included in the tree model.

We will fit the BART model in the lab. This only takes a minute or so. Some of the post-processing for partial dependence plots is slower. We will skip this. These steps were run in advance and output from these steps will be used in the lab.



## Setup

Load the packages used in this lab.

```{r load packages, message=FALSE}
## load required libraries 
#install.packages("corrplot")
library(corrplot)
#install.packages("ggplot2")
library(ggplot2)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("BART")
library(BART)
#install.packages("randomForest")
library(randomForest) 
```

### Uncomment the below and then load bartmix library when running for first time

This package has some functions to post-process the MCMC from BART models. You can adapt the code in this package to meet other estimation needs with BART.

```{r load bartmix, message=FALSE}
# install.packages("devtools")
# devtools::install_github("AnderWilson/bartmix")
library(bartmix)
```

## Load and format data

Load in the data and do some data preparation. The packages used here (`randomForests` and `BART`) can handle cateogrical predictors. We therefore do less data preparation because we do not need to make dummy variables.

```{r load data}
## read in data and only consider complete data 
## this drops 327 individuals
## some tree methods handle missing data but we will not deal with that here
nhanes <- na.omit(read.csv(paste0(here::here(),"/Data/studypop.csv")))
```

```{r format outcome}
## our y variable - ln transformed and scaled mean telomere length
## some tree methods make assumptions about the error variance and some do not
## generally safer to transform the dependent variable to reduce heteroskedasticity
lnLTL_z <- scale(log(nhanes$TELOMEAN)) 
```

```{r format exposures}
## exposures matrix
## most tree methods are robust to skew and outliers in the predictors
## in this case we long transform and 
mixture <- with(nhanes, cbind(LBX074LA, LBX099LA, LBX118LA, LBX138LA, LBX153LA, LBX170LA, LBX180LA, LBX187LA, LBX194LA, LBXHXCLA, LBXPCBLA,
                              LBXD03LA, LBXD05LA, LBXD07LA,
                              LBXF03LA, LBXF04LA, LBXF05LA, LBXF08LA)) 
# Most tree models are invariant the transformations of the predictors (they don't make a difference)
# here we transform it to make the plots consistent with other methods so they can be compared
mixture   <- apply(mixture, 2, log)
mixture <- scale(mixture)
colnames(mixture) <- c(paste0("PCB",c(74, 99, 118, 138, 153, 170, 180, 187, 194, 169, 126)), 
                           paste0("Dioxin",1:3), paste0("Furan",1:4)) 
exposure_names <- colnames(mixture)
```

```{r format covariates}
## our X matrix
covariates <- with(nhanes, cbind(age_cent, male, bmi_cat3, edu_cat, race_cat,
                                 LBXWBCSI, LBXLYPCT, LBXMOPCT, 
                                 LBXNEPCT, LBXEOPCT, LBXBAPCT, ln_lbxcot)) 
```



## Random Forests


### Regress out covariates

For the random forest analysis we will regress the covariates out prior to fitting the random forest model. This model can be fit by including the covariates in the random forest model instead. The BART analysis provides an example of this.

```{r regress out covariates}
# regress covariates out
lnLTL_z_residuals <- lm(lnLTL_z~covariates)$residuals
```


### Fit random forest

The key hyperparameters here are the number of trees (`mtree`) and the number of variables used in each tree (`mtry`). These hyperparameters could be selected with cross validation and/or sensitivity analyses can be used.

```{r fit random forest}
# fit the random forest model
set.seed(1000)
fit_rf <- randomForest(y=lnLTL_z_residuals,
                       x=mixture,
                       ntree=1000,
                       mtry=6,
                       importance = TRUE) 
```

### Prediction


```{r predict with random forests}
pred_rf <- predict(fit_rf)

# view predicted vs observed
plot(pred_rf~lnLTL_z_residuals, 
     main="Predicted vs observed with random forest")
```



### Variable importance

There are many measures of variable importance in trees and none is perfect. Two standard ones for random forests are:
- Node Purity: how much splits on a variable improve model fit (reduce MSE for continuous outcomes)
- Percent increase in MSE: a permutation approach that is normalized out of bag prediction error
In both cases higher is more important.

```{r variable importance with random forests quick plot}
varImpPlot(fit_rf, main = "Random forest important variables", type=1)
```


```{r variable importance with random forests}
# extract variable importance from the fit random forest model
rf_var_imortance <- importance(fit_rf)
rf_var_imortance
```


### Partial dependence plots

The partial dependence plot shows the main effect of one exposure averaged over values of the other exposures. In this example we are only averaging over the other mixture covariates. We are not allowing for heterogeneity based on covariates because they were regressed out prior to estimating the random forest model. Note that there is no measure of uncertainty (no confidence intervals).

```{r random forest partial dependence plot for Furan 1 }
partialPlot(x = fit_rf, 
            pred.data = mixture, 
            x.var = "Furan1",
            main = "Funan 1 partial dependence with random forest", 
            xlab = "Exposure (z-score of log-transformed exposure)",
            ylab = "Estimate (mean response)") 
```




## BART

### Fit BART

```{r fit BART model}
# fit the BART model
set.seed(1000)
fit_bart <- gbart(x.train=cbind(mixture,covariates), 
                  y.train=lnLTL_z,
                  nskip=2000,    # MCMC iterations that are discarded as burning
                  ndpost=2000)   # MCMC iterations after burning that are retained for 

load(paste0(here::here(), "/Supervised/Tree Based Methods/bart_fits_for_lab.rda"))
```
### Fitted values

Fitted values in BART are the estiamted mean response at the observed values. Note that this is very different that the random forest plot because the covariates have not been regressed out of the outcome.

```{r fitted values}
plot(fit_bart$yhat.train.mean~lnLTL_z, 
     main="fitted vs observed with BART")
```


### Variable importance

A simple measure of variable importance in BART is how many trees each variable is included in. We get an estimate of this with uncertainty. A variable being included in a lot of trees does not necessarily mean the variable has a large association or an association at all.

BART can be extended to do variable selection (use `sparse=TRUE`). Variable selection will be applied to all predictors including covariates. It may be more appropriate when covariates are regressed out of the outcome. This is likely to lead to more useful measures of variable importance.

The `bartMachines` package has additional variable importance measures.

```{r BART variable importance}
# variable importance for the mixture components with BART
varcount <- data.frame(exposure=exposure_names,
                       mean=fit_bart$varcount.mean[exposure_names],
                       lower=apply(fit_bart$varcount[,exposure_names],2,quantile,0.025),
                       upper=apply(fit_bart$varcount[,exposure_names],2,quantile,0.975))

# visualize the variable importance with BART
ggplot(varcount, aes(x=exposure, y=mean, ymin=lower, ymax=upper)) +
  geom_errorbar(width=0) +
  geom_point() +
  theme_minimal() + 
  ggtitle("Variable Inclusion Count with BART") + 
  xlab("Exposure") + 
  ylab("Count") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

### Partial dependence

The partial dependence functions with BART include uncertainty.

```{r BART partial dependence for Furan 1, eval=FALSE}
# partial dependence of BART
# funan1_pd <- partialdependence1(fit_bart, 
#                                 data=cbind(mixture,covariates), 
#                                 exposures = "Furan1", 
#                                 L=50) 
```

```{r BART partial dependence for Furan 1 visualization}
ggplot(funan1_pd, aes(x=x, y=mean, ymin=lower, ymax=upper)) + 
  geom_ribbon(fill="grey70") + 
  geom_line() + 
  theme_minimal() + 
  ggtitle("Funan 1 partial dependence with BART") + 
  xlab("Exposure (z-score of log-transformed exposure)") + 
  ylab("Estimate (mean response)") 
```

```{r BART partial dependence for all components, echo=FALSE, eval=FALSE}
# all_pd <- partialdependence1(fit_bart,
#                             data=cbind(mixture,covariates),
#                             exposures = exposure_names,
#                             L=50)
```

```{r BART partial dependence for all components visualization}
plt <- all_pd %>%
mutate(exposure = fct_recode(exposure, "PCB 74" = "PCB74",
                                "PCB 99" = "PCB99",
                                "PCB 118" = "PCB118",
                                "PCB 138" = "PCB138",
                                "PCB 153" = "PCB153",
                                "PCB 170" = "PCB170",
                                "PCB 180" = "PCB180",
                                "PCB 187" = "PCB187",
                                "PCB 194" = "PCB194",
                                "1,2,3,6,7,8-hxcdd" = "Dioxin1",
                                "1,2,3,4,6,7,8-hpcdd" = "Dioxin2",
                               "1,2,3,4,6,7,8,9-ocdd" =  "Dioxin3",
                               "2,3,4,7,8-pncdf" =  "Furan1",
                               "1,2,3,4,7,8-hxcdf" =  "Furan2",
                               "1,2,3,6,7,8-hxcdf" =  "Furan3",
                               "1,2,3,4,6,7,8-hxcdf" =  "Furan4",
                               "PCB 169" =  "PCB169",
                                "PCB 126" = "PCB126")) %>% 
  ggplot(aes(x=x, y=mean, ymin=lower, ymax=upper)) +
  facet_wrap(~exposure) +
  geom_ribbon(fill="grey70") +
  geom_line() +
  theme_minimal() +
  ggtitle("Partial dependence with BART") +
  xlab("Exposure (z-score of log-transformed exposure)") +
  ylab("Estimate (mean response)")
plt
```

### 2-way partial dependence

One form of a 2-way partial dependence plot shows the partial dependence of one exposure at different quantiles of another exposure. In this case, the partial dependence plot for PCB169 is shown at five quantiles of Furan 1. There is no evidence of interaction so the lines are exactly on top of each other.

```{r two way partial dependence with BART, eval=FALSE}
# pd_2way <- partialdependence2(fit_bart, 
#                                 data=cbind(mixture,covariates),
#                                 var = "PCB169", 
#                               var2 = "Furan1",
#                               qtls = c(0.1,0.25,0.5,0.75,0.9),
#                                 L=20)
```


```{r two way partial dependence with BART visualize}
pd_2way$qtl <- as.factor(pd_2way$qtl)
ggplot(pd_2way, aes(x=x, y=mean, 
                    color=qtl,
                    linetype=qtl)) + 
  geom_line() + 
  theme_minimal() + 
  ggtitle("PCB169 partial dependence by quantile of Furan 1 with BART") + 
  xlab("Exposure (z-score of log-transformed exposure)") + 
  ylab("Estimate (mean response)") 
```


### Total mixture effect

We can estimate the total mixture effect in the same way that is typical with BKMR. This analysis estimates the mean response when all covariates are set to a specific quantile. In this case each quantile from 0.2 to 0.8 in steps of 0.05. 


```{r BART total mixture effect, eval=FALSE}
# totalmix <- totalmixtureeffect(fit_bart, 
#                                data=cbind(mixture,covariates), 
#                                exposures = exposure_names,
#                                qtls = seq(0.2,0.8,0.05))
```

```{r BART total mixture effect visualization}
ggplot(totalmix, aes(x=quantile, y=mean, ymin=lower, ymax=upper)) + 
  geom_errorbar(width=0) + geom_point() + theme_minimal() +
  ggtitle("Total mixture effect with BART") + 
  xlab("Quantile") + 
  ylab("Estimate (mean response)")
```



```{r save results, include=FALSE, purl=FALSE}
# this is used to save results from the full run
# save(fit_bart, funan1_pd, all_pd, totalmix, pd_2way, file="bart_fits_for_lab.rda")
```

