---
title: "Tree Based Methods"
author: "Ahlam Abuawad"
date: "7/10/2020"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("caret")
library(caret)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("pROC")
library(pROC)
#install.packages("rpart")
library(rpart)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("tree")
library(tree)
#install.packages("ranger")
library(ranger)
#install.packages("randomForest")
library(randomForest)
#install.packages("gbm")
library(gbm)
#install.packages("knitr")
library(knitr)
```

# Data Import and Cleaning 

First, load the dataset; clean up names as needed; and convert factors to, well, factors. 

```{r}
study_pop = read_csv(here::here("Data/studypop.csv")) %>% 
  clean_names(case = c("old_janitor")) %>% 
  mutate(bmi_cat3 = as.factor(bmi_cat3),
         edu_cat = as.factor(edu_cat),
         race_cat = as.factor(race_cat),
         male = as.factor(male)) 
```

Quick data descriptions; because of the length of the output, we don't execute this command here, but encourage you to do so!

```{r, eval = FALSE}
#describe(study_pop)
```

Next we remove missing values and reorder predictors (environmental variables first, confounders second). In keeping with standard practice, we'll ln-transform the environmental exposures and the outcome. 

```{r}
data_tree = study_pop %>% 
  mutate_at(vars(contains("la")), log) %>% 
  mutate(log_telomean = log(telomean)) %>% 
  dplyr::select(log_telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn, -telomean) %>% 
  na.omit(log_telomean) 

names(data_tree)

dim(data_tree)
```

Trees utilize a training set to model set parameters that can be applied to a test set (remaining observations). 

```{r}
set.seed(1993) # for every cv step

# creating training set
train_telo <- sample(1:nrow(data_tree), floor(nrow(data_tree)/2))
data_train <- data_tree[train_telo,]

# creating test set
data_test <- data_tree[-train_telo,]
```

Let's take a quick look at our two datasets.

```{r}
dim(data_train) # 501 observations
dim(data_test) # 502 observations

#View(data_train)
```

# Regression Trees

We'll start by fitting a regression tree to the training data, with log telomere length as the response and all other variables as predictors. We'll use cross-validation (CV) on the training set to determine the optimal tree size.

```{r}
### Fitting a single tree using Recursive Partitioning and Regression Trees (rpart)
fit <- rpart(formula = log_telomean ~ ., 
             data = data_train,
             control = rpart.control(cp = 0.005))

# cp - Complexity Parameter; Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. Essentially,the user decides that any split that doesn't improve the fit by cp will likely be pruned off by CV.

# Display the Complexity Parameter table for fitted rpart
cpTab <- printcp(fit)
knitr::kable(cpTab)

# Visual representation of CV results from rpart
plotcp(fit, col = "red") # red horizontal line is drawn 1 SE above the minimum of the curve (min error)
```

We can look at the tree with minimal CV error (and the best tree using the 1 SE rule).

```{r}
# Determining the size of the tree with min CV error
minErr <- which.min(cpTab[, 4]) # 3

# Creating a tree based on the minErr
tree_best <- prune(fit, cp = cpTab[minErr, 1])
tree_best

# Checking for trees with minimum complexity
good_telo <- which(cpTab[, 4] < cpTab[minErr, 4] + cpTab[minErr, 5]) # 2, 3, 4
min_complexity_telo <- good_telo[1] # 2

# Creating tree based on 1 SE rule
tree_1se <- prune(fit, cp = cpTab[min_complexity_telo, 1])
```

We can explore the results looking at a plot of the tree.

```{r}
# Tree based on 1 SE rule
rpart.plot(tree_1se)

# Tree based on minimal CV error
rpart.plot(tree_best)
```

We can measure "how important" each variable is for the final result, and calculate the mean squared error (MSE) of the tree result using the testing dataset. 

```{r}
# Variable importance - Represents the relative contribution of each input variable in prediction
tree_1se$variable.importance/max(tree_1se$variable.importance)
tree_best$variable.importance/max(tree_best$variable.importance)

# Predict values onto test set
preds_1se <- predict(tree_1se, data_test)
preds_best <- predict(tree_best, data_test)

# Average & sd log telomean length
mean(data_test$log_telomean)
sd(data_test$log_telomean)

# Calculate the mean squared error (MSE)
(MSE_1tree_1se <- mean((preds_1se - data_test$log_telomean)^2))
(MSE_1tree_1best <- mean((preds_best - data_test$log_telomean)^2))
```


# Bagging

We'll perform bagging on the training set, and then use the importance() function to determine which variables are most important. 

```{r bag}
set.seed(1993)
bag_telo = randomForest(log_telomean ~ ., 
                        data = data_train,
                        mtry = 31, 
                        importance = TRUE) 

# mtry should equal number of variables, in bagging there is no random selection of variables at each split (that's the difference with random forests)

# View variable importance based on model
importance(bag_telo)

# Quickly plot variable importance 
varImpPlot(bag_telo, main = "Bagging Important Variables", type = 1)

# Create dataset based on variable importance
bag_imp_df <- importance(bag_telo) %>% 
  data.frame() %>% 
  mutate(feature = row.names(.))
  
# Remove confounders
bag_imp_df <- bag_imp_df[-c(18:31), ] 

# Plot the variable importance of chemicals
bag_imp_df %>% 
  ggplot(aes(x = reorder(feature, X.IncMSE), 
                         y = X.IncMSE)) +
  geom_point() +
  coord_flip() +
  theme_classic() +
  theme(axis.text.y = element_text(hjust = 1),
        plot.title = element_text(hjust = 0.5)) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_line(colour = 'gray', linetype = 'dashed'),
        panel.background = element_rect(fill = 'white', colour = 'black')) +
    labs(x = "Chemical",
         y = "% Increase in MSE",
         title = "Bagging Important Variables")

# Predict values onto test set
preds_bag = predict(bag_telo, newdata = data_test)

# Calculate the MSE
(MSE_bag <- mean((preds_bag - data_test$log_telomean)^2))
```


# Random Forests

Increasing randomness when aggregating (decorrelating the trees) decreases noise. Here we will try to randomly select 6 variables at each split

```{r}
# Fitting a random forest using randomForest function
fit_rf <- randomForest(log_telomean ~ ., 
                       data = data_train,
                       mtry = 6,
                       importance = TRUE)

## Note: this is the same function as for bagging, but now mtry << input variables

# View variable importance based on model
importance(fit_rf, type=1)

# Quickly plot variable importance 
varImpPlot(fit_rf, main = "Random Forest Important Variables", type=1)

# Create dataset based on variable importance
rf_imp_df <- importance(fit_rf) %>% 
  data.frame() %>% 
  mutate(feature = row.names(.))
  

# Remove confounders
rf_imp_df <- bag_imp_df[-c(18:31), ] 

# Plot the important chemicals
rf_imp_df %>% 
  ggplot(aes(x = reorder(feature, X.IncMSE), 
                         y = X.IncMSE)) +
  geom_point() +
  coord_flip() +
  theme_classic() +
  theme(axis.text.y = element_text(hjust = 1),
        plot.title = element_text(hjust = 0.5)) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_line(colour = 'gray', linetype = 'dashed'),
        panel.background = element_rect(fill = 'white', colour = 'black')) +
    labs(x = "Chemical",
         y = "% Increase in MSE",
         title = "Random Forest Important Variables")

# Fitting a random forest using a different function
fit_ranger <- ranger(log_telomean ~ ., 
                       data = data_train,
                     min.node.size = 20,
                     num.trees = 1000)
fit_ranger

# Predict values onto test set
preds_rf <- predict(fit_rf, data_test)
preds_ranger <- predict(fit_ranger, data_test)$predictions

# View observed vs predicted values from ranger random forest
data_test %>% 
  ggplot(aes(x = preds_ranger, y = log_telomean)) +
  geom_point(colour = "#ff6767", alpha = 0.3) +
  theme_bw(18) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(title = "Predicted vs Observed", 
       x = "Predicted Telomere Length", 
       y = "Oberved Telomere Length")

# Calculate MSE
(MSE_rf <- mean((preds_rf - data_test$log_telomean)^2))
(MSE_ranger <- mean((preds_ranger - data_test$log_telomean)^2))
```


# Comparison of Tree-Based Models

Compare the error rate of different tree-based models 

```{r compare}
test_MSE = round(cbind(MSE_1tree_1se, MSE_1tree_1best, MSE_bag, MSE_rf, MSE_ranger),4)

rownames(test_MSE) = colnames(MSE_1tree_1se); colnames(test_MSE) = c("Tree 1 SE", "Tree Best", "Bagging", "Random Forest", "Ranger")

knitr::kable(test_MSE, align = "c")
```

Traditional classical tree-based methods are not able to control for confounders outside the tree part (i.e. the tree will split both confounders and exposure variables, making results difficult to compare to other methods discussed in this workshop). 

Additionally, the lack of coefficients/exposure response curves makes it difficult to interpret the results (magnitude and direction of association). There are methods (e.g., g-computation type methods) that allow reconstruction of E-R curves.
